#!/usr/bin/env python

from ActionSpace import ActionSpace
from RewardPair import RewardPair
from Environment import Mines
from Environment.Mines import Mine_Data
from random import randint
from Agents import Action_Definition
import time
import xxhash

import math
import numpy as np

class Solver: 


	def __init__(self,max_depth_,depth_,Gamma_,upper_confidence_c_,action_space_num_,map_size_):

		self.upper_confidence_c=upper_confidence_c_		
		self.action_space_num=action_space_num_

		self.N = {}
		self.Na= {}
		self.Q = {}


		self.max_depth=max_depth_
		self.depth=depth_
		self.reward_list=[]
		self.environment_data =Mine_Data(map_size_)#
		self.Gamma=Gamma_

	def GetGreedyPrimitive(t,h):
		if t is primitive:
			return t
		else:
			return getGreedyPrimitive(get_best_action,h)

	def getPrimitive(t,h):
		if t is primitive:
			return t
		else
			return getPrimitive(t,h)

	
	
	def OnlinePlanning(self,search_tree_,agent_,environment_data_,num_steps_,a_):
		for i in range(0, num_steps_):
			agent_.imprint(a_)
			environment_data_.imprint(self.environment_data)

			self.search(root_task,self.environment_data,a_.get_hash_history(),self.depth,T,abf())


	def search(self,t,s,h,d,T,abf):

	def get_best_action(self, search_tree_, agent_, environment_data_):
		self.pre_hash_key=self.hash_generator(agent_,environment_data_)
		if self.pre_hash_key in search_tree_:
			return search_tree_.get(self.pre_hash_key).get_best_action().get_action_index()
		else:
			print "Missing Hash"



	def p_info(self, action_space_):
		print "printing rewards"
		for k,rp in action_space_.get_reward_pairs().items():
			print "action: ", Action_Definition.action_to_string(rp.get_action_index())
			print "visited: ", rp.get_visited(), "reward: ", rp.get_estimated_reward()
			print ""

	def get_best_action_and_step(self, agent_, environment_data_):
		
		self.pre_hash_key=self.hash_generator(agent_,environment_data_)
		if self.pre_hash_key in self.hash:
			agent_.step(self.hash.get(self.pre_hash_key).get_best_action().get_action_index(),environment_data_,False)
			agent_.update_history(self.hash.get(self.pre_hash_key).get_best_action().get_action_index(),self.hash_generator_without_history(agent_,environment_data_))
			#print self.p_info(self.hash.get(self.pre_hash_key))
		else:
			print "Missing Hash"
			print len(self.hash)
			for k,v in self.hash.items():
				print self.pre_hash_key
				print k
				print v

	def arg_max_ucb(self,agent_,t,h):
		a = 0
		max= self.Q.get(self.hash_generator(t,h,0)) + c*math.sqrt(math.log(self.N.get(self.hash_generator(t,h)))/self.Na.get(self.hash_generator(t,h,0)) 
		for i in range(0, agent_.get_action_space_num_):
			if self.Q.get(self.hash_generator(t,h,0)) + c*math.sqrt(math.log(self.N.get(self.hash_generator(t,h)))/self.Na.get(self.hash_generator(t,h,0))  > max:
				a=i
				max=self.Q.get(self.hash_generator(t,h,0)) + c*math.sqrt(math.log(self.N.get(self.hash_generator(t,h)))/self.Na.get(self.hash_generator(t,h,0)) 

		return a
			

	def search(self,t,s,h,d,T,abf):
		if t is primitive:
			(s1,r) = agent.simulate(t,s)
			#simulate
			#get abstracted state from abf()
			x = abf(s1)
			return (r,1,h++xxhash.xxh64(t).hexdigest()++xxhash.xxh64(x).hexdigest(),s1)
		else:
			if d>=H or t terminates at h:
				return (0,0,h,s)
			else:
				self.pre_hash_key= self.hash_generator(t,h)
				if  self.pre_hash_key not in T:
					T[self.pre_hash_key] = 1
					return Rollout(t,s,h,d,abf)
				else:
					a_star = self.arg_max_ucb(agent_,t,h)
					(r1,n1,h1,s1)= search(a_star,s,h,d,T,abf)
					(r2,n2,h2,s2)=search(t,s1,h1,d+n1,T,abf)

					#if self.hash_generator(t,h) in self.N:
					self.N[self.hash_generator(t,h)] +=1
					#else:
					#	self.N[self.hash_generator(t,h)] = 1

					#if self.hash_generator(t,h,a_star) in self.Na:
					self.Na[self.hash_generator(t,h,a_star)] +=1
					#else:
				#		self.Na[self.hash_generator(t,h,a_star)] = 1


					r= r1+math.pow(Gamma,n1)*r2
               				self.Q[self.hash_generator(t,h,a_star)] = self.Q[self.hash_generator(t,h,a_star)] + (r-self.Q[self.hash_generator(t,h,a_star)])/self.Na[self.hash_generator(t,h,a_star)]
					return (r,n1+n2,h2,s2)




	def rollout(self, agent_, environment_data_,num_steps):
		for i in range(0, num_steps):
			base_reward = environment_data_.get_reward()
			agent_.simulate(randint(0,agent_.get_action_size()),environment_data_)
			self.reward_list.append(environment_data_.get_reward()-base_reward)
			
	

	def hash_generator(self,t,h):
		return task.get_hash()+h

	def hash_generator(self,t,h,a):
		return task.get_hash()+h+xxhash.xxh64(a).hexdigest()


	def hash_generator(self, agent_, environment_data_):	
		#self.pre_result = 1
		
		#self.pre_result = self.pre_result*self.pre_prime + environment_data_.get_hash()
		#self.pre_result = self.pre_result*self.pre_prime + agent_.get_hash()
		#self.pre_result = self.pre_result*self.pre_prime + agent_.get_hash_history()
		
		return environment_data_.get_hash()+agent_.get_hash()+ agent_.get_hash_history()

	def hash_generator_without_history(self, agent_, environment_data_):	
		#self.pre_result = 1
		
		#self.pre_result = self.pre_result*self.pre_prime + environment_data_.get_hash()
		#self.pre_result = self.pre_result*self.pre_prime + agent_.get_hash()

		return environment_data_.get_hash()+agent_.get_hash()


